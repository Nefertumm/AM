{"cells":[{"cell_type":"code","execution_count":50,"id":"bb1115eb-e872-4cf7-87d9-da4adf9c2d21","metadata":{"id":"bb1115eb-e872-4cf7-87d9-da4adf9c2d21"},"outputs":[],"source":["import numpy as np\n","import matplotlib as mpl\n","import matplotlib.pyplot as plt"]},{"cell_type":"markdown","id":"7f44ba4e","metadata":{"id":"7f44ba4e"},"source":["## Dataset MNIST\n","\n","El dataset **MNIST** consiste en imágenes de números manuscritos entre 0 y 9. Las imágenes tienen dimensiones de 28x28 pixeles y cada pixel está representado por un valor de intensidad en escala de grises. El conjunto de entrenamiento consiste en 60000 dígitos y el conjunto de prueba de 10000.\n","\n","### Implemente una CNN para obtener un desempeño de al menos 99% en el conjunto de prueba"]},{"cell_type":"code","execution_count":60,"id":"643230e3","metadata":{},"outputs":[],"source":["#Importamos las librerias y la base de datos MNIST\n","import tensorflow as tf\n","from tensorflow.keras import layers, models\n","from tensorflow.keras.datasets import mnist"]},{"cell_type":"code","execution_count":61,"id":"5bb95991","metadata":{},"outputs":[],"source":["# Cargar el dataset MNIST\n","(train_images, train_labels), (test_images, test_labels) = mnist.load_data()"]},{"cell_type":"code","execution_count":62,"id":"6274e268","metadata":{},"outputs":[],"source":["# Preprocesamos los datos\n","# El reshape se realiza para agregar una dimension de color al dataset, en este caso es 1 porque es escala de grises\n","train_images = train_images.reshape((60000, 28, 28, 1))\n","test_images = test_images.reshape((10000, 28, 28, 1))\n","# Ahora se normalizan los valores de los pixeles de 0 a 1, se divide por 255 que es el valor maximo de un pixel\n","train_images, test_images = train_images / 255.0, test_images / 255.0"]},{"cell_type":"code","execution_count":63,"id":"526ca132","metadata":{},"outputs":[],"source":["# Data Augmentation\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","\n","datagen = ImageDataGenerator(rotation_range=10, width_shift_range=0.1, height_shift_range=0.1, zoom_range=0.1)\n","datagen.fit(train_images)"]},{"cell_type":"code","execution_count":64,"id":"9c39fcdd","metadata":{},"outputs":[],"source":["# Definimos la red neuronal convolucional\n","# Sequential permite crear modelos capa por capa de forma secuencial\n","model = models.Sequential([\n","    # Capa convolucional, 32 filtros de 3x3, funcion de activacion relu, input_shape es el tamaño de la imagen\n","    # Kerner_regularizer se utiliza para evitar el overfitting, penaliza los pesos grandes\n","    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1), kernel_regularizer=tf.keras.regularizers.l2(0.01)),\n","    # Capa de pooling, se utiliza para reducir las dimensiones espaciales de salida, por ende el número de parámetros de la red\n","    layers.MaxPooling2D((2, 2)),\n","    # Capa convolucional, 16 filtros de 3x3, funcion de activacion relu\n","    layers.Conv2D(64, (3, 3), activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.01)),\n","    layers.MaxPooling2D((2, 2)),\n","    layers.Conv2D(64, (3, 3), activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.01)),\n","    # La repetición de los parámetros de arriba se debe a que se busca aumentar la profundidad de la red\n","    # Capa flatten, se utiliza para aplanar la entrada, se utiliza para pasar de una capa convolucional a una densa\n","    layers.Flatten(),\n","    layers.Dropout(0.3),\n","    # Capa densa, 32 neuronas, funcion de activacion relu\n","    layers.Dense(32, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.01)),\n","    # Capa densa, 10 neuronas, funcion de activacion softmax, esta última se utiliza para problemas de clasificación multiclase\n","    layers.Dense(10, activation='softmax')\n","])"]},{"cell_type":"code","execution_count":65,"id":"6c2870f7","metadata":{},"outputs":[],"source":["# Early stopping, callback de la función fit, se utiliza para detener el entrenamiento cuando se alcanza un valor de perdida\n","# no tolerable, en este caso se utiliza para evitar el overfitting\n","from tensorflow.keras.callbacks import EarlyStopping\n","\n","early_stopping = EarlyStopping(monitor='loss', patience=5)"]},{"cell_type":"code","execution_count":66,"id":"cf43478d","metadata":{},"outputs":[],"source":["# Compilamos el modelo\n","optimizer = tf.keras.optimizers.RMSprop(learning_rate=0.0001)\n","# sparse_categorical_crossentropy se utiliza para problemas de clasificación multiclase\n","model.compile(optimizer=optimizer,\n","              loss='sparse_categorical_crossentropy',\n","              metrics=['accuracy'])\n"]},{"cell_type":"code","execution_count":67,"id":"e708253c","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/30\n","1875/1875 [==============================] - 13s 7ms/step - loss: 2.0536 - accuracy: 0.6066\n","Epoch 2/30\n","1875/1875 [==============================] - 12s 6ms/step - loss: 1.0962 - accuracy: 0.8547\n","Epoch 3/30\n","1875/1875 [==============================] - 12s 6ms/step - loss: 0.8551 - accuracy: 0.8944\n","Epoch 4/30\n","1875/1875 [==============================] - 12s 7ms/step - loss: 0.7294 - accuracy: 0.9087\n","Epoch 5/30\n","1875/1875 [==============================] - 12s 7ms/step - loss: 0.6528 - accuracy: 0.9180\n","Epoch 6/30\n","1875/1875 [==============================] - 12s 7ms/step - loss: 0.6025 - accuracy: 0.9235\n","Epoch 7/30\n","1875/1875 [==============================] - 12s 7ms/step - loss: 0.5670 - accuracy: 0.9263\n","Epoch 8/30\n","1875/1875 [==============================] - 12s 7ms/step - loss: 0.5463 - accuracy: 0.9285\n","Epoch 9/30\n","1875/1875 [==============================] - 12s 7ms/step - loss: 0.5239 - accuracy: 0.9318\n","Epoch 10/30\n","1875/1875 [==============================] - 12s 7ms/step - loss: 0.5109 - accuracy: 0.9327\n","Epoch 11/30\n","1875/1875 [==============================] - 12s 7ms/step - loss: 0.5006 - accuracy: 0.9342\n","Epoch 12/30\n","1875/1875 [==============================] - 12s 7ms/step - loss: 0.4874 - accuracy: 0.9364\n","Epoch 13/30\n","1875/1875 [==============================] - 12s 7ms/step - loss: 0.4825 - accuracy: 0.9355\n","Epoch 14/30\n","1875/1875 [==============================] - 12s 7ms/step - loss: 0.4732 - accuracy: 0.9374\n","Epoch 15/30\n","1875/1875 [==============================] - 12s 7ms/step - loss: 0.4654 - accuracy: 0.9383\n","Epoch 16/30\n","1875/1875 [==============================] - 12s 6ms/step - loss: 0.4640 - accuracy: 0.9381\n","Epoch 17/30\n","1875/1875 [==============================] - 12s 7ms/step - loss: 0.4572 - accuracy: 0.9400\n","Epoch 18/30\n","1875/1875 [==============================] - 12s 7ms/step - loss: 0.4510 - accuracy: 0.9399\n","Epoch 19/30\n","1875/1875 [==============================] - 12s 7ms/step - loss: 0.4457 - accuracy: 0.9412\n","Epoch 20/30\n","1875/1875 [==============================] - 12s 7ms/step - loss: 0.4425 - accuracy: 0.9408\n","Epoch 21/30\n","1875/1875 [==============================] - 12s 7ms/step - loss: 0.4392 - accuracy: 0.9420\n","Epoch 22/30\n","1875/1875 [==============================] - 12s 7ms/step - loss: 0.4335 - accuracy: 0.9420\n","Epoch 23/30\n","1875/1875 [==============================] - 12s 7ms/step - loss: 0.4323 - accuracy: 0.9420\n","Epoch 24/30\n","1875/1875 [==============================] - 12s 7ms/step - loss: 0.4268 - accuracy: 0.9439\n","Epoch 25/30\n","1875/1875 [==============================] - 12s 7ms/step - loss: 0.4252 - accuracy: 0.9433\n","Epoch 26/30\n","1875/1875 [==============================] - 12s 7ms/step - loss: 0.4205 - accuracy: 0.9446\n","Epoch 27/30\n","1875/1875 [==============================] - 12s 7ms/step - loss: 0.4219 - accuracy: 0.9438\n","Epoch 28/30\n","1875/1875 [==============================] - 12s 7ms/step - loss: 0.4130 - accuracy: 0.9464\n","Epoch 29/30\n","1875/1875 [==============================] - 12s 7ms/step - loss: 0.4127 - accuracy: 0.9456\n","Epoch 30/30\n","1875/1875 [==============================] - 12s 7ms/step - loss: 0.4090 - accuracy: 0.9454\n"]},{"data":{"text/plain":["<keras.src.callbacks.History at 0x23ccd7ba5d0>"]},"execution_count":67,"metadata":{},"output_type":"execute_result"}],"source":["# Entrenamos el modelo\n","model.fit(datagen.flow(train_images, train_labels, batch_size=32), epochs=30, callbacks=[early_stopping])"]},{"cell_type":"code","execution_count":68,"id":"6b1ccb18","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["313/313 [==============================] - 1s 2ms/step - loss: 0.2973 - accuracy: 0.9779\n","Accuracy en el conjunto de prueba: 97.79%\n"]}],"source":["# Evaluamos el modelo en el conjunto de prueba\n","test_loss, test_acc = model.evaluate(test_images, test_labels)\n","print(f'Accuracy en el conjunto de prueba: {test_acc*100:.2f}%')"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.4"}},"nbformat":4,"nbformat_minor":5}
